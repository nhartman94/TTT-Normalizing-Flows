{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "28dce70f-aead-4ec3-8da3-27c51d6f8e14",
   "metadata": {},
   "source": [
    "# Real-NVPs\n",
    "\n",
    "**Goal:** Implement a Real-NVP model on your own and evaluate the results on some **toy problem**.\n",
    "\n",
    "![](flow-graphic.png)\n",
    "\n",
    "\n",
    "**Table of Contents:**\n",
    "1. [Dataset](#data) \n",
    "2. [Model architecture](#model)\n",
    "- Q1: Implement the t (translation) NN in the `CouplingLayer` \n",
    "- Q2: Implement the forward function n the `CouplingLayer` \n",
    "- Q3: Implement the log(det(Jacobian)) for the reverse function\n",
    "3. [Training](#train)\n",
    "- Q4: Add the loss function definition to the `train` function\n",
    "- Q5: Visualize the density of $f^{-1}(X)$ for a trained model\n",
    "4. [Visualizations](#viz)\n",
    "- Code already provided, just for fun to look at what the step-by-step flow denisty propogation is doing.\n",
    "5. [Bonus: Conditional generative model](#condFlow)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfeee921-3883-4226-9077-42fba5894a11",
   "metadata": {},
   "source": [
    "## 1. Dataset\n",
    "<a name=\"data\"></a>\n",
    "\n",
    "Let's use the crescent moons dataset as a nice 2d test bed example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56509fd8-7bc7-4334-93a7-96891a19e804",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_moons\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "import math\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28601f80-bd51-49e3-9747-3113e7ed09be",
   "metadata": {},
   "outputs": [],
   "source": [
    "nsamples = 30_000\n",
    "noise = 0.05\n",
    "X = make_moons(nsamples, noise=noise)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e6d1178-38ca-4f52-8514-0191cd7f1980",
   "metadata": {},
   "outputs": [],
   "source": [
    "nViz=500\n",
    "\n",
    "plt.scatter(X[:nViz,0], X[:nViz,1])\n",
    "\n",
    "plt.xlabel('$X_0$',fontsize=18)\n",
    "plt.ylabel('$X_1$',fontsize=18)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "011f96e8-885e-4ff2-afde-b2bf4fd2fb57",
   "metadata": {},
   "source": [
    "## 2. Model architecture\n",
    "<a name=\"model\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64e19360-ea25-4f9e-b874-7dc644bb3919",
   "metadata": {},
   "source": [
    "### Implementation detail: Masking\n",
    "\n",
    "In the lecture, we talked about the affine transformations transforming a bipartite grouping of the variables (via \"coupling layers\").\n",
    "\n",
    "I.e, for $f: x \\rightarrow y, x,y \\in \\mathbb{R}^n$, we group the variables into the <span style=\"color:deeppink\">transforming dimensions</span> and <span style=\"color:blue\">conditioning dimensions</span>, and just transfom one set of variables at a time, while leaving the others stationary, e.g,\n",
    "\n",
    "$$ y_{1:d} = x_{1:d} $$\n",
    "$$y_{d+1:D} = x_{d+1:D} \\cdot s(x_{1:d})  + t(x_{1:d})$$\n",
    "\n",
    "(and in the next step of the flow $x_{1:d}$ would get transformed while $x_{d+1:D}$ would be useds for the conditioning).\n",
    "\n",
    "As described in [1605.08803](https://arxiv.org/abs/1605.08803), it's common to implement the variable partitioning with a binary mask, here denoted $m$, where\n",
    "$$\n",
    "m_i = \n",
    "\\begin{cases}\n",
    "0, \\quad \\text{transforming dimension} \\\\\n",
    "1, \\quad \\text{conditioning dimension}\n",
    "\\end{cases}\n",
    "$$\n",
    "This allows us to implement the the update eqs for a single flow step more succinctly as:\n",
    "\n",
    "$$y =  m \\odot x + (1-m) \\odot \\left[x \\cdot s(m \\odot x )  + t(m \\odot x ) \\right].$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97daecb5-76b3-42f8-9726-1e5cd078cc87",
   "metadata": {},
   "source": [
    "We simplify a little bit to try to get into a more succinct form.\n",
    "\n",
    "- Write in terms of `log_s` (as this is the NN we have).\n",
    "\n",
    "$$y =  m \\odot x + (1-m) \\odot \\left[x \\cdot \\exp \\log s(m \\odot x )  + t(m \\odot x ) \\right]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73ed8a3c-99c0-49fc-8155-a5f8492ac2e9",
   "metadata": {},
   "source": [
    "- Group the terms with $x$ together.\n",
    "\n",
    "$$y =  x \\odot \\left[ m  + (1-m) \\cdot \\exp \\log s(m \\odot x ) \\right]  + t(m \\odot x ) \\cdot (1-m)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac335e15-f4a6-4ce5-a11b-4166f4c8922e",
   "metadata": {},
   "source": [
    "We want to take the `exp` outside of the expression in $[ \\ldots ]$.\n",
    "\n",
    "Note that\n",
    "\n",
    "$$\\exp(m_i) = \n",
    "1, \\  \\ \\text{if} \\ m_i = 0 .$$\n",
    "\n",
    "\n",
    "$$y =  x \\odot  \\exp \\left[( \\log s(m \\odot x ) \\cdot (1-m)  \\right]  + t(m \\odot x ) \\cdot (1-m)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "232dc17b-c0ab-455c-958a-2c98b3dfca6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CouplingLayer(nn.Module):\n",
    "    \"\"\" An implementation of a coupling layer\n",
    "    from RealNVP (https://arxiv.org/abs/1605.08803).\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, num_inputs, num_hidden, mask):\n",
    "        super(CouplingLayer, self).__init__()\n",
    "\n",
    "        self.num_inputs = num_inputs\n",
    "        self.mask = mask\n",
    "            \n",
    "        self.log_s_net = nn.Sequential(\n",
    "            nn.Linear(num_inputs, num_hidden), nn.Tanh(),\n",
    "            nn.Linear(num_hidden, num_hidden), nn.Tanh(),\n",
    "            nn.Linear(num_hidden, num_inputs))\n",
    "        \n",
    "        '''\n",
    "        TO DO (Q1): Setup the network for the shift of the affine transformation\n",
    "        - Use 2 hidden layers\n",
    "        - `num_hidden` units / hidden layer\n",
    "        - ReLU activation\n",
    "        '''\n",
    "        self.t_net = \n",
    "\n",
    "    def forward(self, inputs,  mode='forward'):\n",
    "        '''\n",
    "        Returns a tuple with \n",
    "        - the transofrmation, f(inpts)\n",
    "        - and the log Jacobian of f\n",
    "\n",
    "        Whether we apply f or f^{-1} depends on whether we're running \n",
    "        with `mode` as forward or reverse\n",
    "        '''\n",
    "        mask = self.mask\n",
    "        masked_inputs = inputs * mask\n",
    "        \n",
    "        if mode == 'forward':\n",
    "            \n",
    "            '''\n",
    "            TO DO (Q2): Implement the transformation\n",
    "            '''\n",
    "            \n",
    "            log_s = \n",
    "            s = \n",
    "            t = \n",
    "            out = \n",
    "            \n",
    "            return out, log_s.sum(-1, keepdim=True)\n",
    "                        \n",
    "        else:\n",
    "            \n",
    "            log_s = self.log_s_net(masked_inputs) * (1 - mask)\n",
    "            t = self.t_net(masked_inputs) * (1 - mask)\n",
    "            s = torch.exp(log_s)\n",
    "            \n",
    "            '''\n",
    "            TO DO (Q3): Implement the Jacobian\n",
    "            '''\n",
    "            log_jacob = \n",
    "            \n",
    "            return (inputs - t) / s, log_jacob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6decdc50-4672-4883-82f3-e31822212e95",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4bd87d1c-e13b-4333-9ef5-8ae9fefce6ae",
   "metadata": {},
   "source": [
    "### Sanity check\n",
    "\n",
    "After working through steps (1) -- (3), the CouplingLayer class should work :) \n",
    "\n",
    "And the code block below let's you check the implementation.\n",
    "\n",
    "![](frogs.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5145cb83-5983-4ad6-9e21-6df298417cdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a random normally distributed vector \n",
    "# with 100 samples in 2 dimensions\n",
    "Z = torch.randn(100,2)\n",
    "\n",
    "# dim 0 will be the conditioning variable\n",
    "# dim 1 will be the transforming variable\n",
    "mask = torch.Tensor([1,0])\n",
    "f = CouplingLayer(2,64,mask)\n",
    "\n",
    "with torch.no_grad():\n",
    "    out, log_p = f(Z)\n",
    "\n",
    "# The histogram shows the inputs and the outputs\n",
    "fig, axes = plt.subplots(1,2,figsize=(8,4))\n",
    "\n",
    "for i, ax, c in zip([0,1],axes,['royalblue','hotpink']):\n",
    "\n",
    "    ax.hist(Z[:,i].numpy(),25,(-3,3),label=f'$Z_{i}$',color=c,alpha=.4)\n",
    "    ax.hist(out[:,i].numpy(),25,(-3,3),label=f'$f(Z_{i})$',color=c,lw=2.5,histtype='step')\n",
    "    \n",
    "    ax.set_xlabel(f'dim {i}',fontsize=18)\n",
    "    ax.legend(fontsize=12)\n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88a2c56a-146e-4413-979c-d637f445a938",
   "metadata": {},
   "source": [
    "^ Is the result what you expected?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b7e74f7-22ad-47a7-ae1e-fd116b8ad45b",
   "metadata": {},
   "source": [
    "The `FlowSequential` then stitches together a sequence of CouplingLayers together to form a single model that can evaluate eitheer\n",
    "\n",
    "For this tutorial we're giving a ready-to-go implementation `FlowSequential` class :)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14cb1ac9-68a4-43af-9156-2653f0cebccd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "424ff37c-4762-48ee-bc72-1858f4b98e5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FlowSequential(nn.Sequential):\n",
    "    \"\"\" A sequential container for flows.\n",
    "    In addition to a forward pass it implements a backward pass and\n",
    "    computes log jacobians.\n",
    "    \"\"\"\n",
    "\n",
    "    def forward(self, inputs, mode='forward'):\n",
    "        \"\"\" Performs a forward or reverse pass for flow modules.\n",
    "        Args:\n",
    "            inputs: a tuple of inputs and logdets\n",
    "            mode: to run direct computation or inverse\n",
    "        \"\"\"\n",
    "        \n",
    "        self.num_inputs = inputs.size(-1)\n",
    "\n",
    "        # Initialize logdets to 0\n",
    "        logdets = torch.zeros(inputs.size(0), 1, device=inputs.device)\n",
    "\n",
    "        assert mode in ['forward', 'reverse']\n",
    "        if mode == 'forward':\n",
    "            for module in self._modules.values():\n",
    "                inputs, logdet = module(inputs, mode)\n",
    "                logdets += logdet\n",
    "        else:\n",
    "            for module in reversed(self._modules.values()):\n",
    "                inputs, logdet = module(inputs, mode)\n",
    "                logdets += logdet\n",
    "\n",
    "        return inputs, logdets\n",
    "\n",
    "    def log_probs(self, inputs):\n",
    "        z, log_jacob = self(inputs,mode='reverse')\n",
    "          \n",
    "        log_probs = (-0.5 * z.pow(2) - 0.5 * math.log(2 * math.pi)).sum(-1, keepdim=True)\n",
    "        return (log_probs + log_jacob).sum(-1, keepdim=True)\n",
    "\n",
    "    def sample(self, num_samples=None):\n",
    "        \n",
    "        noise = torch.Tensor(num_samples, self.num_inputs).normal_()\n",
    "        noise = noise.to(device)\n",
    "\n",
    "        samples = self.forward(noise, mode='forward')[0]\n",
    "        \n",
    "        return samples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd93e4d0-670a-46bd-83dd-6b49e4ff182c",
   "metadata": {},
   "source": [
    "### Flow architecture\n",
    "\n",
    "Let's consider a flow with 9 blocks and the $\\log(s)$ and $t$ NNs with 64 hidden units per hidden layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1df3633f-2913-4a09-a20a-9324e5485604",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_blocks=9\n",
    "num_hidden=64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94920430-33e7-4c48-aec7-49cd5e38f080",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Check for a GPU\n",
    "if torch.cuda.is_available():\n",
    "    device = 'cuda:0'\n",
    "else:\n",
    "    device='cpu'\n",
    "    \n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abb47986-e87e-49dd-aaeb-cb133a34a5f9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "num_inputs = X.shape[1]\n",
    "\n",
    "mask = torch.arange(0, num_inputs) % 2\n",
    "mask = mask.to(device).float()\n",
    "\n",
    "modules=[]\n",
    "\n",
    "for _ in range(num_blocks):\n",
    "    modules.append( CouplingLayer( num_inputs, num_hidden, mask) )\n",
    "    mask = 1 - mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2739677d-0f6f-4a26-a9ae-cc68ed04209d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = FlowSequential(*modules)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ebd1b45-43b4-4e56-b24a-03b3672aa989",
   "metadata": {},
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a1b5169-5417-46c8-8b6d-3198a8c37431",
   "metadata": {},
   "source": [
    "## 4. Training\n",
    "<a name=\"train\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "755595ef-14da-47cb-bea2-72f16f643177",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_torch = torch.Tensor(X.astype(np.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dc001a0-0f4f-4eaf-af4a-7c9e45aca4d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size=256\n",
    "\n",
    "nTrain=24_000\n",
    "nVal=3_000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daa62787-ba8d-42f7-87ef-8cee24f16a57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For comparison with a later cell block ... look at the density \n",
    "# of the _test data_ before training the odel\n",
    "\n",
    "model = model.to(device)\n",
    "with torch.no_grad():\n",
    "    Z_test = model(X_torch[nTrain+nVal:].to(device),mode='reverse')[0].cpu().numpy()\n",
    "\n",
    "plt.figure(figsize=(5,5))\n",
    "\n",
    "plt.scatter(*Z_test.T)\n",
    "\n",
    "plt.xlim(-3,3)\n",
    "plt.ylim(-3,3)\n",
    "\n",
    "plt.xlabel('$Z_0$',fontsize=20)\n",
    "plt.ylabel('$Z_1$',fontsize=20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d97de27-4828-4b9d-a217-b50a8d38f2b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "kwargs = {'num_workers': 4, 'pin_memory': True}  if torch.cuda.is_available() else {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5fcac01-7ccb-491d-a2d6-38a16bdf7f02",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(\n",
    "    X_torch[:nTrain], batch_size=batch_size, shuffle=True,**kwargs)\n",
    "\n",
    "valid_loader = torch.utils.data.DataLoader(\n",
    "    X_torch[nTrain : nTrain+nVal],\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    "    drop_last=False,\n",
    "    **kwargs)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    X_torch[nTrain+nVal :],\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    "    drop_last=False,\n",
    "    **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c37fe80-1dfe-4cf4-8860-8643500c13e1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train(model, train_loader):\n",
    "    \n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "\n",
    "    for batch_idx, data in enumerate(train_loader):\n",
    "   \n",
    "        data = data.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        '''\n",
    "        TO DO (Q4): Fill in the expression for the loss\n",
    "        '''\n",
    "        loss = \n",
    "        train_loss += loss.item()\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "    train_loss /= len(train_loader)\n",
    "    return train_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a386305a-88fd-468f-98da-662b599fea35",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def validate(model, loader, prefix='Validation'):\n",
    "    \n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "\n",
    "    for batch_idx, data in enumerate(loader):\n",
    "\n",
    "        data = data.to(device)\n",
    "        with torch.no_grad():\n",
    "            val_loss += -model.log_probs(data).sum().item()  # sum up batch loss\n",
    "     \n",
    "    val_loss /= len(loader.dataset)\n",
    "    return val_loss "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94c57c25-eaa5-411f-bf18-839699b2a230",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr=1e-4\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=1e-6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1932d869-1387-4240-8fd6-e776eb31aba9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_model(model, cond_input=None, title=''):\n",
    "    '''\n",
    "    Goal: Given the given model weights, show\n",
    "    (1) The density\n",
    "    (2) Samples from p_X(x)\n",
    "    \n",
    "    Inputs:\n",
    "    - model that we're plotting the density of\n",
    "    - cond_input: Extra logic for the conditional gen model\n",
    "                  (bonus prob)\n",
    "    - title: Super title over 2 subfigs\n",
    "    '''\n",
    "    \n",
    "    fig, (ax1,ax2) = plt.subplots(1,2,figsize=(10,4),\n",
    "                                  gridspec_kw={'hspace':10})\n",
    "\n",
    "    # Title the plot with the log(p) on the validation set\n",
    "    if title:\n",
    "        fig.suptitle(title)\n",
    "\n",
    "    '''\n",
    "    (1) Plot the density\n",
    "    '''\n",
    "    x = np.linspace(-1,2)\n",
    "    y = np.linspace(-.75,1.25)\n",
    "\n",
    "    xx,yy = np.meshgrid(x,y)\n",
    "\n",
    "    X_grid = np.vstack([xx.flatten(),yy.flatten()]).T.astype(np.float32)\n",
    "    X_grid.T\n",
    "\n",
    "    X_grid = torch.tensor(X_grid).to(device)\n",
    "\n",
    "    if cond_input:\n",
    "        # For last (bonus) prob\n",
    "        y0,y1 = cond_input\n",
    "        Y_grid = torch.ones_like(X_grid).to(device)\n",
    "        Y_grid[:,0] = y0\n",
    "        Y_grid[:,1] = y1\n",
    "    \n",
    "        with torch.no_grad():\n",
    "            log_probs = model.log_probs(X_grid,Y_grid).cpu().numpy()\n",
    "\n",
    "    else:\n",
    "        with torch.no_grad():\n",
    "            log_probs = model.log_probs(X_grid).cpu().numpy()\n",
    "\n",
    "    ax1.pcolormesh(xx,yy,np.exp(log_probs.reshape(50,50)),shading='auto',cmap='coolwarm')\n",
    "\n",
    "    ax1.set_xlabel('$X_0$',fontsize=12)\n",
    "    ax1.set_ylabel('$X_1$',fontsize=12)\n",
    "\n",
    "    '''\n",
    "    (2) Plot samples from the model\n",
    "    '''\n",
    "    with torch.no_grad():\n",
    "        \n",
    "        if cond_input:\n",
    "            # For last (bonus) prob\n",
    "            Y_con = torch.ones(500,2).to(device)\n",
    "            Y_con[:,0] = y0\n",
    "            Y_con[:,1] = y1\n",
    "            X_gen = model.sample(Y_con).cpu().numpy()\n",
    "        else:\n",
    "            X_gen = model.sample(500).cpu().numpy()\n",
    "        \n",
    "    ax2.scatter(*X_gen.T)\n",
    "\n",
    "    ax2.set_xlabel('$X_0$',fontsize=12)\n",
    "    ax2.set_ylabel('$X_1$',fontsize=12)\n",
    "\n",
    "    ax2.set_xlim(x[[0,-1]])\n",
    "    ax2.set_ylim(y[[0,-1]])\n",
    "\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf89d963-00ea-4e17-bd65-04a24ce30861",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs=51\n",
    "\n",
    "train_losses = np.zeros(epochs)\n",
    "val_losses = np.zeros(epochs)\n",
    "\n",
    "\n",
    "for i in range(epochs):\n",
    "    \n",
    "    train_losses[i] = train(model, train_loader)\n",
    "    val_losses[i] = validate(model, valid_loader)\n",
    "\n",
    "    print(f'Epoch {i}: train loss = {train_losses[i]:.4f}, val loss = {val_losses[i]:.4f}')\n",
    "        \n",
    "    if i % 10 == 0:\n",
    "        # Draw the model prediction\n",
    "        draw_model(model,title=f'Validation loss = {val_losses[i]}')\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d195f903-bc8d-44cd-a4c8-e3b3848f7ea3",
   "metadata": {},
   "source": [
    "Plot the training and validation loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44ff1540-dc94-4b13-affd-54312667c024",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(train_losses,label='train')\n",
    "plt.plot(val_losses,label='val')\n",
    "\n",
    "plt.xlabel('epochs',fontsize=20)\n",
    "plt.ylabel('Loss',fontsize=20)\n",
    "plt.legend(fontsize=15)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8189258-be50-43b0-b85e-761b2714b4ae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a7057d40-fe82-405f-894e-b1fb65538507",
   "metadata": {},
   "source": [
    "**What's the density of the test data _after_ training?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60a0d928-e2b4-4d84-81d2-67e498c9c77b",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "TO DO (Q5): Get the (2d) density of the test data\n",
    "'''\n",
    "\n",
    "with torch.no_grad():\n",
    "    Z_test = \n",
    "\n",
    "plt.figure(figsize=(5,5))\n",
    "\n",
    "plt.scatter(*Z_test.T)\n",
    "\n",
    "plt.xlim(-3,3)\n",
    "plt.ylim(-3,3)\n",
    "\n",
    "plt.xlabel('$Z_0$',fontsize=20)\n",
    "plt.ylabel('$Z_1$',fontsize=20)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcbb7a57-cdb3-48da-948b-3dbbbc74a5cf",
   "metadata": {},
   "source": [
    "Is this what we expect for the trained flow?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ba47baa-dd29-4b14-90fc-e846e911f3d9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "eebb4f5e-7bc0-416d-8202-c515875b7aef",
   "metadata": {},
   "source": [
    "## 4. Vizualize the flow\n",
    "<a name=\"viz\"></a>\n",
    "\n",
    "This was a 9 step flow, so let's sample some noise, and look at each of the 10 steps of the flow that goes into the crescent moon density :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cbf0351-a90b-4971-a61b-9c0c3adcdf7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 500\n",
    "Z = torch.randn(N,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4ca71e1-57db-47c4-b81c-90260fc3c270",
   "metadata": {},
   "outputs": [],
   "source": [
    "cmask = np.where((Z[:,0]<0) & (Z[:,1] >0),0,\n",
    "                np.where((Z[:,0]>0) & (Z[:,1] >0),1,\n",
    "                        np.where((Z[:,0]>0) & (Z[:,1]<0),2,3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f537c6b-c465-460a-8014-47a8d00db233",
   "metadata": {},
   "outputs": [],
   "source": [
    "colors = [f'C{i}' for i in cmask ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1421a094-8224-4ab5-9ef4-dc85295d8e79",
   "metadata": {},
   "outputs": [],
   "source": [
    "nrows = 2\n",
    "ncols = 5\n",
    "\n",
    "fig, ax = plt.subplots(nrows,ncols,figsize=(16,9))\n",
    "\n",
    "ax[0,0].scatter(*Z.T, color=colors)\n",
    "ax[0,0].set_title('Norm(0,1)',fontsize=14)\n",
    "\n",
    "'''\n",
    "Let each subsequent step of the flow viz the jazz\n",
    "'''\n",
    "\n",
    "Z_gpu = Z.float().to(device)\n",
    "\n",
    "for k,module in enumerate(modules):\n",
    "    \n",
    "    with torch.no_grad():    \n",
    "        Z_gpu, _ = module(Z_gpu)\n",
    "    \n",
    "    \n",
    "    i = (k+1) // ncols\n",
    "    j = (k+1) % ncols\n",
    "    \n",
    "    ax[i,j].set_title(f'Flow step {k+1}',fontsize=14)\n",
    "    \n",
    "    ax[i,j].scatter(*Z_gpu.cpu().numpy().T, color=colors)\n",
    "      \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "393675f1-4f13-465d-913f-01b6d5f75ac8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03a0c709-698f-4deb-8144-5e666244ac6f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ab43a879-89e1-4180-bbd4-8e8ecfc4b098",
   "metadata": {},
   "source": [
    "# 5. Bonus: Conditional flow\n",
    "\n",
    "Lots of applications in science involve conditional flow models, can you extend our the model we built here to be conditioned on the center point for the moon? \n",
    "\n",
    "**Plan:** \n",
    "- Train sampling moon density centers $y \\in \\mathbb{R}^2$ **uniformly** from [0,1]\n",
    "\n",
    "Recall, we were training the 30k training samples for modelling 2 dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9cb9143-a095-4d01-b319-cc0135029a28",
   "metadata": {},
   "outputs": [],
   "source": [
    "nsamples,num_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39f41476-7a75-4633-9972-96d903aad439",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = torch.Tensor(nsamples, num_inputs).uniform_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd8c71f8-bf3e-406b-8023-3078180d6f85",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_cond = X_torch + Y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc93a15b-eba1-4d9d-92aa-925d096f2e80",
   "metadata": {},
   "source": [
    "**Let's look at the prediction for some slices of the conditional output**\n",
    "- $0 < y_0 < .05$ and $0 < y_1 < .05$\n",
    "- $0.5 < y_0 < .55$ and $.5 < y_1 < .55$\n",
    "- $0.5 < y_0 < .55$ and $.5 < y_1 < .55$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c475180-7ecc-4091-92b2-8dc75a9d3ee2",
   "metadata": {},
   "outputs": [],
   "source": [
    "dy = .05\n",
    "for i,y_min in enumerate([0,.5,1-dy]):\n",
    "    \n",
    "    y_max = y_min+dy\n",
    "    mi = (Y[:,0] > y_min) & (Y[:,0] < y_max)\n",
    "    mi = mi & (Y[:,1] > y_min) & (Y[:,1] < y_max)\n",
    "    \n",
    "    y_avg = .5 * (y_min + y_max)\n",
    "    \n",
    "    c=f'C{i}'\n",
    "    plt.scatter(*X_cond[mi].T.numpy(),alpha=.5,color=c)\n",
    "    plt.scatter([y_avg],[y_avg],250,marker='x',label='avg center',color=c)\n",
    "    plt.xlabel('$X_0$',fontsize=18)\n",
    "    plt.ylabel('$X_1$',fontsize=18)\n",
    "    plt.legend()\n",
    "    plt.title(f'$y_0,y_1$ center in ({y_min},{y_max})')\n",
    "    \n",
    "    plt.xlim(-1.2,3.2)\n",
    "    plt.ylim(-.7,2.2)\n",
    "    plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c02ab76b-2f5a-441f-8668-76cf34819aa3",
   "metadata": {},
   "source": [
    "### TO DO: Implement the conditional flow model\n",
    "\n",
    "**Hint 1:** Extend the coupling layer to a conditional coupling layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03202eff-c689-4f4b-9bd8-b5356c330717",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CondCouplingLayer(nn.Module):\n",
    "\n",
    "    def __init__(self, num_inputs, num_cond_inputs, num_hidden, mask):\n",
    "        super(CondCouplingLayer, self).__init__()\n",
    "\n",
    "        '''\n",
    "        TO DO: Fill in \n",
    "        (Tip: look at `Coupling layer` for inspiration)\n",
    "        '''\n",
    "\n",
    "    def forward(self, inputs, cond_inputs,  mode='forward'):\n",
    "        \n",
    "        mask = self.mask\n",
    "        masked_inputs = inputs * mask\n",
    "        \n",
    "        all_inputs = torch.cat([masked_inputs, cond_inputs],axis=1)\n",
    "        \n",
    "        if mode == 'forward':\n",
    "            \n",
    "            '''\n",
    "            TO DO : Fill in \n",
    "            '''\n",
    "            raise NotImplementedError  \n",
    "                \n",
    "        else:\n",
    "            \n",
    "            '''\n",
    "            TO DO : Fill in \n",
    "            '''\n",
    "            raise NotImplementedError  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce1ac38e-6fc5-4508-948f-f83a27a714b8",
   "metadata": {},
   "source": [
    "**Hint 2:** Implement a `CondFlowSequential` class that calls the `CondCouplingLayer` class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52ea20b1-f828-4c31-a9c9-7de0e30bcf7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CondFlowSequential(nn.Sequential):\n",
    "    \"\"\" A sequential container for flows extending the \n",
    "    FlowSequential class for conditional inputs\n",
    "    \"\"\"\n",
    "\n",
    "    def forward(self, inputs, cond_inputs, mode='forward'):\n",
    "        \"\"\" Performs a forward or reverse pass for flow modules.\n",
    "        Args:\n",
    "            inputs: a tuple of inputs and logdets\n",
    "            cond_inputs: The conditional inputs to the flow\n",
    "            mode: to run direct computation or inverse\n",
    "        \"\"\"\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def log_probs(self, inputs, cond_inputs):\n",
    "        raise NotImplementedError\n",
    "        \n",
    "    def sample(self, cond_inputs, num_samples=None):\n",
    "        \n",
    "        raise NotImplementedError"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af877d7d-ec25-4631-aa5c-c970fec0ca3a",
   "metadata": {},
   "source": [
    "**Put the pieces together to define a flow**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "844c65a3-7b6c-4980-babc-c7536336a49b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extending the code that we had before\n",
    "num_cond_inputs = Y.shape[1]\n",
    "\n",
    "modules=[]\n",
    "\n",
    "for _ in range(num_blocks):\n",
    "    modules.append( CondCouplingLayer( num_inputs, num_cond_inputs, num_hidden, mask) )\n",
    "    mask = 1 - mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "249c7b96-048b-4a1e-8c84-37f5d6f7b612",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To DO: Define the conditional flow from these modules\n",
    "cond_flow = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a63ed817-a4e9-4e99-9f59-fcd7af971784",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Once you've defined the model... put it on the gpu\n",
    "cond_flow = cond_flow.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1a77025-e17e-4586-964d-0761f9054f3a",
   "metadata": {},
   "source": [
    "**Training code**\n",
    "\n",
    "It's the same as before, just evaluating the conditional generative model, so we'll just give you the training functions.\n",
    "\n",
    "We'll just use a `from torch.utils.data.Dataset` first to pass to the `DataLoaders` that we'll create :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d2adfda-6efa-4bd4-bfe5-ca570717b809",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "New dataset class to deal w/ the tuple of X,Y input\n",
    "'''\n",
    "\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class CondMoonsDataset(Dataset):\n",
    "    '''\n",
    "    Skeleton class taken from:\n",
    "    https://pytorch.org/tutorials/beginner/basics/data_tutorial.html\n",
    "    '''\n",
    "    def __init__(self,nExamples,noise=0.05):\n",
    "        \n",
    "        X_np = make_moons(nExamples, noise=noise)[0]\n",
    "        X = torch.Tensor(X_np).float()\n",
    "            \n",
    "        num_inputs = X.shape[1]\n",
    "        Y = torch.Tensor(nExamples, num_inputs).uniform_()\n",
    "         \n",
    "        X = X+Y\n",
    "            \n",
    "        self.X = X\n",
    "        self.Y = Y\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.X.shape[0]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.Y[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0375ae1-3461-4c12-a4b0-43141bdc2350",
   "metadata": {},
   "outputs": [],
   "source": [
    "cond_train_loader = torch.utils.data.DataLoader(\n",
    "    CondMoonsDataset(nTrain), \n",
    "    batch_size=batch_size, shuffle=True,**kwargs)\n",
    "\n",
    "cond_valid_loader = torch.utils.data.DataLoader(\n",
    "    CondMoonsDataset(nVal),\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    "    drop_last=False,\n",
    "    **kwargs)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    CondMoonsDataset(nVal),\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    "    drop_last=False,\n",
    "    **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40ce6336-8ad4-491e-aea1-ed5c76e96ff2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cond_train(model, train_loader, opt):\n",
    "    \n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "\n",
    "    for batch_idx, (X,Y) in enumerate(train_loader):\n",
    "        \n",
    "        X = X.to(device) \n",
    "        Y = Y.to(device) \n",
    "        opt.zero_grad()\n",
    "        \n",
    "        loss = -model.log_probs(X,Y).mean()\n",
    "        train_loss += loss.item()\n",
    "        \n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        \n",
    "    train_loss /= len(train_loader)\n",
    "    return train_loss\n",
    "\n",
    "def cond_validate(model, loader, prefix='Validation'):\n",
    "    \n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "\n",
    "    for batch_idx, (X,Y) in enumerate(loader):\n",
    "\n",
    "        X = X.to(device) \n",
    "        Y = Y.to(device) \n",
    "            \n",
    "        with torch.no_grad():\n",
    "            val_loss += -model.log_probs(X,Y).sum().item()  # sum up batch loss\n",
    "     \n",
    "    val_loss /= len(loader.dataset)\n",
    "    return val_loss "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "782c1e64-8073-4faa-9c9a-52c6f0b1b03e",
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = torch.optim.Adam(cond_flow.parameters(), lr=lr, weight_decay=1e-6)\n",
    "\n",
    "epochs=51\n",
    "\n",
    "cond_train_losses = np.zeros(epochs)\n",
    "cond_val_losses = np.zeros(epochs)\n",
    "\n",
    "for i in range(epochs):\n",
    "    \n",
    "    cond_train_losses[i] = cond_train(cond_flow, cond_train_loader, opt)\n",
    "    cond_val_losses[i] = cond_validate(cond_flow, cond_valid_loader)\n",
    "\n",
    "    print(f'Epoch {i}: train loss = {cond_train_losses[i]:.4f}, val loss = {cond_val_losses[i]:.4f}')\n",
    "        \n",
    "    if i % 10 == 0:\n",
    "        # Draw the model prediction\n",
    "        draw_model(cond_flow,cond_input=[0,0],\n",
    "                   title=f'Validation loss = {cond_val_losses[i]}')\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "640a574c-bc96-4002-9846-da7c8d856e12",
   "metadata": {},
   "source": [
    "**Sanity check:** What have we learned about crescent moons in the three cases that we had above?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a27e004d-99ea-4ac6-bb5c-1bc9a74b9cd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1,3,figsize=(12,3),\n",
    "                            gridspec_kw={'wspace':.3})\n",
    "\n",
    "fig.suptitle('Conditional flow')\n",
    "\n",
    "# Define the density grid\n",
    "x = np.linspace(-1.2,3.2)\n",
    "y = np.linspace(-.7,2.2)\n",
    "\n",
    "xx,yy = np.meshgrid(x,y)\n",
    "\n",
    "X_grid = np.vstack([xx.flatten(),yy.flatten()]).T.astype(np.float32)\n",
    "X_grid.T\n",
    "\n",
    "X_grid = torch.tensor(X_grid).to(device)\n",
    "\n",
    "'''\n",
    "Loop over the centers\n",
    "'''\n",
    "for yi,ax,cmap in zip([0, 0.5, 1],axs,['Blues','Oranges','Greens']):\n",
    "    \n",
    "    # Plot the density\n",
    "    \n",
    "    Y_grid = torch.ones_like(X_grid).to(device)\n",
    "    Y_grid[:,0] *= yi\n",
    "    Y_grid[:,1] *= yi\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        log_probs = cond_flow.log_probs(X_grid,Y_grid).cpu().numpy()\n",
    "\n",
    "    ax.pcolormesh(xx,yy,np.exp(log_probs.reshape(50,50)),shading='auto',cmap=cmap)\n",
    "\n",
    "    ax.set_xlabel('$X_0$',fontsize=12)\n",
    "    ax.set_ylabel('$X_1$',fontsize=12)\n",
    "\n",
    "    ax.scatter([yi],[yi],300,marker='x',color='k')\n",
    "    # break\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2af543b4-26fa-4e6e-8d1a-ef3ff75eb203",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "453b6b8b-5b2c-47f2-a73a-088aa5a34aab",
   "metadata": {},
   "source": [
    "**Resources:**\n",
    "- This tutorial for the code in this repo came from the [pytorch-flows](https://github.com/ikostrikov/pytorch-flows) repo.\n",
    "- The [nflows](https://github.com/bayesiains/nflows.git) is also a very nice package that includes the Real-NVP model and also the RQ-NSF that we also talked about in the lecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "728dc6cf-fb65-429a-9119-648d6095b2df",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
