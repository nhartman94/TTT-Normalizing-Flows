{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "28dce70f-aead-4ec3-8da3-27c51d6f8e14",
   "metadata": {},
   "source": [
    "# Real-NVPs\n",
    "\n",
    "**Goal:** Implement a Real-NVP model on your own and evaluate the results on some **toy problem**.\n",
    "\n",
    "![](flow-graphic.png)\n",
    "\n",
    "\n",
    "**Table of Contents:**\n",
    "1. [Dataset](#data) \n",
    "2. [Model architecture](#model)\n",
    "- Q1: Implement the t (translation) NN in the `CouplingLayer` \n",
    "- Q2: Implement the forward function n the `CouplingLayer` \n",
    "- Q3: Implement the log(det(Jacobian)) for the reverse function\n",
    "3. [Training](#train)\n",
    "- Q4: Add the loss function definition to the `train` function\n",
    "- Q5: Visualize the density of $f^{-1}(X)$ for a trained model\n",
    "4. [Visualizations](#viz)\n",
    "- Code already provided, just for fun to look at what the step-by-step flow denisty propogation is doing."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfeee921-3883-4226-9077-42fba5894a11",
   "metadata": {},
   "source": [
    "## 1. Dataset\n",
    "<a name=\"data\"></a>\n",
    "\n",
    "Let's use the crescent moons dataset as a nice 2d test bed example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56509fd8-7bc7-4334-93a7-96891a19e804",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_moons\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "import math\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28601f80-bd51-49e3-9747-3113e7ed09be",
   "metadata": {},
   "outputs": [],
   "source": [
    "nsamples = 30_000\n",
    "noise = 0.05\n",
    "X = make_moons(nsamples, noise=noise)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e6d1178-38ca-4f52-8514-0191cd7f1980",
   "metadata": {},
   "outputs": [],
   "source": [
    "nViz=500\n",
    "\n",
    "plt.scatter(X[:nViz,0], X[:nViz,1])\n",
    "\n",
    "plt.xlabel('$X_0$',fontsize=18)\n",
    "plt.ylabel('$X_1$',fontsize=18)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "011f96e8-885e-4ff2-afde-b2bf4fd2fb57",
   "metadata": {},
   "source": [
    "## 2. Model architecture\n",
    "<a name=\"model\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64e19360-ea25-4f9e-b874-7dc644bb3919",
   "metadata": {},
   "source": [
    "### Implementation detail: Masking\n",
    "\n",
    "In the lecture, we talked about the affine transformations transforming a bipartite grouping of the variables (via \"coupling layers\").\n",
    "\n",
    "I.e, for $f: x \\rightarrow y, x,y \\in \\mathbb{R}^n$, we group the variables into the <span style=\"color:deeppink\">transforming dimensions</span> and <span style=\"color:blue\">conditioning dimensions</span>, and just transfom one set of variables at a time, while leaving the others stationary, e.g,\n",
    "\n",
    "$$ y_{1:d} = x_{1:d} $$\n",
    "$$y_{d+1:D} = x_{d+1:D} \\cdot s(x_{1:d})  + t(x_{1:d})$$\n",
    "\n",
    "(and in the next step of the flow $x_{1:d}$ would get transformed while $x_{d+1:D}$ would be useds for the conditioning).\n",
    "\n",
    "As described in [1605.08803](https://arxiv.org/abs/1605.08803), it's common to implement the variable partitioning with a binary mask, here denoted $m$, where\n",
    "$$\n",
    "m_i = \n",
    "\\begin{cases}\n",
    "0, \\quad \\text{transforming dimension} \\\\\n",
    "1, \\quad \\text{conditioning dimension}\n",
    "\\end{cases}\n",
    "$$\n",
    "This allows us to implement the the update eqs for a single flow step more succinctly as:\n",
    "\n",
    "$$y =  m \\odot x + (1-m) \\odot \\left[x \\cdot s(m \\odot x )  + t(m \\odot x ) \\right].$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97daecb5-76b3-42f8-9726-1e5cd078cc87",
   "metadata": {},
   "source": [
    "We simplify a little bit to try to get into a more succinct form.\n",
    "\n",
    "- Write in terms of `log_s` (as this is the NN we have).\n",
    "\n",
    "$$y =  m \\odot x + (1-m) \\odot \\left[x \\cdot \\exp \\log s(m \\odot x )  + t(m \\odot x ) \\right]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73ed8a3c-99c0-49fc-8155-a5f8492ac2e9",
   "metadata": {},
   "source": [
    "- Group the terms with $x$ together.\n",
    "\n",
    "$$y =  x \\odot \\left[ m  + (1-m) \\cdot \\exp \\log s(m \\odot x ) \\right]  + t(m \\odot x ) \\cdot (1-m)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac335e15-f4a6-4ce5-a11b-4166f4c8922e",
   "metadata": {},
   "source": [
    "We want to take the `exp` outside of the expression in $[ \\ldots ]$.\n",
    "\n",
    "Note that\n",
    "\n",
    "$$\\exp(m_i) = \n",
    "1, \\  \\ \\text{if} \\ m_i = 0 .$$\n",
    "\n",
    "\n",
    "$$y =  x \\odot  \\exp \\left[( \\log s(m \\odot x ) \\cdot (1-m)  \\right]  + t(m \\odot x ) \\cdot (1-m)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "232dc17b-c0ab-455c-958a-2c98b3dfca6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CouplingLayer(nn.Module):\n",
    "    \"\"\" An implementation of a coupling layer\n",
    "    from RealNVP (https://arxiv.org/abs/1605.08803).\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, num_inputs, num_hidden, mask):\n",
    "        super(CouplingLayer, self).__init__()\n",
    "\n",
    "        self.num_inputs = num_inputs\n",
    "        self.mask = mask\n",
    "            \n",
    "        self.log_s_net = nn.Sequential(\n",
    "            nn.Linear(num_inputs, num_hidden), nn.Tanh(),\n",
    "            nn.Linear(num_hidden, num_hidden), nn.Tanh(),\n",
    "            nn.Linear(num_hidden, num_inputs))\n",
    "        \n",
    "        '''\n",
    "        TO DO (Q1): Setup the network for the shift of the affine transformation\n",
    "        - Use 2 hidden layers\n",
    "        - `num_hidden` units / hidden layer\n",
    "        - ReLU activation\n",
    "        '''\n",
    "        self.t_net = \n",
    "\n",
    "    def forward(self, inputs,  mode='forward'):\n",
    "        '''\n",
    "        Returns a tuple with \n",
    "        - the transofrmation, f(inpts)\n",
    "        - and the log Jacobian of f\n",
    "\n",
    "        Whether we apply f or f^{-1} depends on whether we're running \n",
    "        with `mode` as forward or reverse\n",
    "        '''\n",
    "        mask = self.mask\n",
    "        masked_inputs = inputs * mask\n",
    "        \n",
    "        if mode == 'forward':\n",
    "            \n",
    "            '''\n",
    "            TO DO (Q2): Implement the transformation\n",
    "            '''\n",
    "            \n",
    "            log_s = \n",
    "            s = \n",
    "            t = \n",
    "            out = \n",
    "            \n",
    "            return out, log_s.sum(-1, keepdim=True)\n",
    "                        \n",
    "        else:\n",
    "            \n",
    "            log_s = self.log_s_net(masked_inputs) * (1 - mask)\n",
    "            t = self.t_net(masked_inputs) * (1 - mask)\n",
    "            s = torch.exp(log_s)\n",
    "            \n",
    "            '''\n",
    "            TO DO (Q3): Implement the Jacobian\n",
    "            '''\n",
    "            log_jacob = \n",
    "            \n",
    "            return (inputs - t) / s, log_jacob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6decdc50-4672-4883-82f3-e31822212e95",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4bd87d1c-e13b-4333-9ef5-8ae9fefce6ae",
   "metadata": {},
   "source": [
    "### Sanity check\n",
    "\n",
    "After working through steps (1) -- (3), the CouplingLayer class should work :) \n",
    "\n",
    "And the code block below let's you check the implementation.\n",
    "\n",
    "![](frogs.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5145cb83-5983-4ad6-9e21-6df298417cdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a random normally distributed vector \n",
    "# with 100 samples in 2 dimensions\n",
    "Z = torch.randn(100,2)\n",
    "\n",
    "# dim 0 will be the conditioning variable\n",
    "# dim 1 will be the transforming variable\n",
    "mask = torch.Tensor([1,0])\n",
    "f = CouplingLayer(2,64,mask)\n",
    "\n",
    "with torch.no_grad():\n",
    "    out, log_p = f(Z)\n",
    "\n",
    "# The histogram shows the inputs and the outputs\n",
    "fig, axes = plt.subplots(1,2,figsize=(8,4))\n",
    "\n",
    "for i, ax, c in zip([0,1],axes,['royalblue','hotpink']):\n",
    "\n",
    "    ax.hist(Z[:,i].numpy(),25,(-3,3),label=f'$Z_{i}$',color=c,alpha=.4)\n",
    "    ax.hist(out[:,i].numpy(),25,(-3,3),label=f'$f(Z_{i})$',color=c,lw=2.5,histtype='step')\n",
    "    \n",
    "    ax.set_xlabel(f'dim {i}',fontsize=18)\n",
    "    ax.legend(fontsize=12)\n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88a2c56a-146e-4413-979c-d637f445a938",
   "metadata": {},
   "source": [
    "^ Is the result what you expected?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b7e74f7-22ad-47a7-ae1e-fd116b8ad45b",
   "metadata": {},
   "source": [
    "The `FlowSequential` then stitches together a sequence of CouplingLayers together to form a single model that can evaluate eitheer\n",
    "\n",
    "For this tutorial we're giving a ready-to-go implementation `FlowSequential` class :)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14cb1ac9-68a4-43af-9156-2653f0cebccd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "424ff37c-4762-48ee-bc72-1858f4b98e5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FlowSequential(nn.Sequential):\n",
    "    \"\"\" A sequential container for flows.\n",
    "    In addition to a forward pass it implements a backward pass and\n",
    "    computes log jacobians.\n",
    "    \"\"\"\n",
    "\n",
    "    def forward(self, inputs, mode='forward'):\n",
    "        \"\"\" Performs a forward or reverse pass for flow modules.\n",
    "        Args:\n",
    "            inputs: a tuple of inputs and logdets\n",
    "            mode: to run direct computation or inverse\n",
    "        \"\"\"\n",
    "        \n",
    "        self.num_inputs = inputs.size(-1)\n",
    "\n",
    "        # Initialize logdets to 0\n",
    "        logdets = torch.zeros(inputs.size(0), 1, device=inputs.device)\n",
    "\n",
    "        assert mode in ['forward', 'reverse']\n",
    "        if mode == 'forward':\n",
    "            for module in self._modules.values():\n",
    "                inputs, logdet = module(inputs, mode)\n",
    "                logdets += logdet\n",
    "        else:\n",
    "            for module in reversed(self._modules.values()):\n",
    "                inputs, logdet = module(inputs, mode)\n",
    "                logdets += logdet\n",
    "\n",
    "        return inputs, logdets\n",
    "\n",
    "    def log_probs(self, inputs):\n",
    "        z, log_jacob = self(inputs,mode='reverse')\n",
    "          \n",
    "        log_probs = (-0.5 * z.pow(2) - 0.5 * math.log(2 * math.pi)).sum(-1, keepdim=True)\n",
    "        return (log_probs + log_jacob).sum(-1, keepdim=True)\n",
    "\n",
    "    def sample(self, num_samples=None):\n",
    "        \n",
    "        noise = torch.Tensor(num_samples, self.num_inputs).normal_()\n",
    "        noise = noise.to(device)\n",
    "\n",
    "        samples = self.forward(noise, mode='forward')[0]\n",
    "        \n",
    "        return samples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd93e4d0-670a-46bd-83dd-6b49e4ff182c",
   "metadata": {},
   "source": [
    "### Flow architecture\n",
    "\n",
    "Let's consider a flow with 9 blocks and the $\\log(s)$ and $t$ NNs with 64 hidden units per hidden layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1df3633f-2913-4a09-a20a-9324e5485604",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_blocks=9\n",
    "num_hidden=64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94920430-33e7-4c48-aec7-49cd5e38f080",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Check for a GPU\n",
    "if torch.cuda.is_available():\n",
    "    device = 'cuda:0'\n",
    "else:\n",
    "    device='cpu'\n",
    "    \n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abb47986-e87e-49dd-aaeb-cb133a34a5f9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "num_inputs = X.shape[1]\n",
    "\n",
    "mask = torch.arange(0, num_inputs) % 2\n",
    "mask = mask.to(device).float()\n",
    "\n",
    "modules=[]\n",
    "\n",
    "for _ in range(num_blocks):\n",
    "    modules.append( CouplingLayer( num_inputs, num_hidden, mask) )\n",
    "    mask = 1 - mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2739677d-0f6f-4a26-a9ae-cc68ed04209d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = FlowSequential(*modules)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ebd1b45-43b4-4e56-b24a-03b3672aa989",
   "metadata": {},
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a1b5169-5417-46c8-8b6d-3198a8c37431",
   "metadata": {},
   "source": [
    "## 4. Training\n",
    "<a name=\"train\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "755595ef-14da-47cb-bea2-72f16f643177",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_torch = torch.Tensor(X.astype(np.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dc001a0-0f4f-4eaf-af4a-7c9e45aca4d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size=256\n",
    "\n",
    "nTrain=24_000\n",
    "nVal=3_000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daa62787-ba8d-42f7-87ef-8cee24f16a57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For comparison with a later cell block ... look at the density \n",
    "# of the _test data_ before training the odel\n",
    "\n",
    "model = model.to(device)\n",
    "with torch.no_grad():\n",
    "    Z_test = model(X_torch[nTrain+nVal:].to(device),mode='reverse')[0].cpu().numpy()\n",
    "\n",
    "plt.figure(figsize=(5,5))\n",
    "\n",
    "plt.scatter(*Z_test.T)\n",
    "\n",
    "plt.xlim(-3,3)\n",
    "plt.ylim(-3,3)\n",
    "\n",
    "plt.xlabel('$Z_0$',fontsize=20)\n",
    "plt.ylabel('$Z_1$',fontsize=20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d97de27-4828-4b9d-a217-b50a8d38f2b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "kwargs = {'num_workers': 4, 'pin_memory': True}  if torch.cuda.is_available() else {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5fcac01-7ccb-491d-a2d6-38a16bdf7f02",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(\n",
    "    X_torch[:nTrain], batch_size=batch_size, shuffle=True,**kwargs)\n",
    "\n",
    "valid_loader = torch.utils.data.DataLoader(\n",
    "    X_torch[nTrain : nTrain+nVal],\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    "    drop_last=False,\n",
    "    **kwargs)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    X_torch[nTrain+nVal :],\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    "    drop_last=False,\n",
    "    **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c37fe80-1dfe-4cf4-8860-8643500c13e1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train(model, train_loader):\n",
    "    \n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "\n",
    "    for batch_idx, data in enumerate(train_loader):\n",
    "   \n",
    "        data = data.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        '''\n",
    "        TO DO (Q4): Fill in the expression for the loss\n",
    "        '''\n",
    "        loss = \n",
    "        train_loss += loss.item()\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "    train_loss /= len(train_loader)\n",
    "    return train_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a386305a-88fd-468f-98da-662b599fea35",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def validate(model, loader, prefix='Validation'):\n",
    "    \n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "\n",
    "    for batch_idx, data in enumerate(loader):\n",
    "\n",
    "        data = data.to(device)\n",
    "        with torch.no_grad():\n",
    "            val_loss += -model.log_probs(data).sum().item()  # sum up batch loss\n",
    "     \n",
    "    val_loss /= len(loader.dataset)\n",
    "    return val_loss "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94c57c25-eaa5-411f-bf18-839699b2a230",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr=1e-4\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=1e-6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1932d869-1387-4240-8fd6-e776eb31aba9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_model(model, title=''):\n",
    "    '''\n",
    "    Goal: Given the given model weights, show\n",
    "    (1) The density\n",
    "    (2) Samples from p_X(x)\n",
    "    \n",
    "    Inputs:\n",
    "    - model that we're plotting the density of\n",
    "    - title: Super title over 2 subfigs\n",
    "    '''\n",
    "    \n",
    "    fig, (ax1,ax2) = plt.subplots(1,2,figsize=(10,4),\n",
    "                                  gridspec_kw={'hspace':10})\n",
    "\n",
    "    # Title the plot with the log(p) on the validation set\n",
    "    if title:\n",
    "        fig.suptitle(title)\n",
    "\n",
    "    '''\n",
    "    (1) Plot the density\n",
    "    '''\n",
    "    x = np.linspace(-1,2)\n",
    "    y = np.linspace(-.75,1.25)\n",
    "\n",
    "    xx,yy = np.meshgrid(x,y)\n",
    "\n",
    "    X_grid = np.vstack([xx.flatten(),yy.flatten()]).T.astype(np.float32)\n",
    "    X_grid.T\n",
    "\n",
    "    X_grid = torch.tensor(X_grid).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        log_probs = model.log_probs(X_grid,Y_grid).cpu().numpy()\n",
    "\n",
    "    else:\n",
    "        with torch.no_grad():\n",
    "            log_probs = model.log_probs(X_grid).cpu().numpy()\n",
    "\n",
    "    ax1.pcolormesh(xx,yy,np.exp(log_probs.reshape(50,50)),shading='auto',cmap='coolwarm')\n",
    "\n",
    "    ax1.set_xlabel('$X_0$',fontsize=12)\n",
    "    ax1.set_ylabel('$X_1$',fontsize=12)\n",
    "\n",
    "    '''\n",
    "    (2) Plot samples from the model\n",
    "    '''\n",
    "    with torch.no_grad():\n",
    "        X_gen = model.sample(500).cpu().numpy()\n",
    "        \n",
    "    ax2.scatter(*X_gen.T)\n",
    "\n",
    "    ax2.set_xlabel('$X_0$',fontsize=12)\n",
    "    ax2.set_ylabel('$X_1$',fontsize=12)\n",
    "\n",
    "    ax2.set_xlim(x[[0,-1]])\n",
    "    ax2.set_ylim(y[[0,-1]])\n",
    "\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf89d963-00ea-4e17-bd65-04a24ce30861",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs=51\n",
    "\n",
    "train_losses = np.zeros(epochs)\n",
    "val_losses = np.zeros(epochs)\n",
    "\n",
    "\n",
    "for i in range(epochs):\n",
    "    \n",
    "    train_losses[i] = train(model, train_loader)\n",
    "    val_losses[i] = validate(model, valid_loader)\n",
    "\n",
    "    print(f'Epoch {i}: train loss = {train_losses[i]:.4f}, val loss = {val_losses[i]:.4f}')\n",
    "        \n",
    "    if i % 10 == 0:\n",
    "        # Draw the model prediction\n",
    "        draw_model(model,title=f'Validation loss = {val_losses[i]}')\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d195f903-bc8d-44cd-a4c8-e3b3848f7ea3",
   "metadata": {},
   "source": [
    "Plot the training and validation loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44ff1540-dc94-4b13-affd-54312667c024",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(train_losses,label='train')\n",
    "plt.plot(val_losses,label='val')\n",
    "\n",
    "plt.xlabel('epochs',fontsize=20)\n",
    "plt.ylabel('Loss',fontsize=20)\n",
    "plt.legend(fontsize=15)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8189258-be50-43b0-b85e-761b2714b4ae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a7057d40-fe82-405f-894e-b1fb65538507",
   "metadata": {},
   "source": [
    "**What's the density of the test data _after_ training?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60a0d928-e2b4-4d84-81d2-67e498c9c77b",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "TO DO (Q5): Get the (2d) density of the test data\n",
    "'''\n",
    "\n",
    "with torch.no_grad():\n",
    "    Z_test = \n",
    "\n",
    "plt.figure(figsize=(5,5))\n",
    "\n",
    "plt.scatter(*Z_test.T)\n",
    "\n",
    "plt.xlim(-3,3)\n",
    "plt.ylim(-3,3)\n",
    "\n",
    "plt.xlabel('$Z_0$',fontsize=20)\n",
    "plt.ylabel('$Z_1$',fontsize=20)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcbb7a57-cdb3-48da-948b-3dbbbc74a5cf",
   "metadata": {},
   "source": [
    "Is this what we expect for the trained flow?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ba47baa-dd29-4b14-90fc-e846e911f3d9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "eebb4f5e-7bc0-416d-8202-c515875b7aef",
   "metadata": {},
   "source": [
    "## 4. Vizualize the flow\n",
    "<a name=\"viz\"></a>\n",
    "\n",
    "This was a 9 step flow, so let's sample some noise, and look at each of the 10 steps of the flow that goes into the crescent moon density :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cbf0351-a90b-4971-a61b-9c0c3adcdf7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 500\n",
    "Z = torch.randn(N,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4ca71e1-57db-47c4-b81c-90260fc3c270",
   "metadata": {},
   "outputs": [],
   "source": [
    "cmask = np.where((Z[:,0]<0) & (Z[:,1] >0),0,\n",
    "                np.where((Z[:,0]>0) & (Z[:,1] >0),1,\n",
    "                        np.where((Z[:,0]>0) & (Z[:,1]<0),2,3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f537c6b-c465-460a-8014-47a8d00db233",
   "metadata": {},
   "outputs": [],
   "source": [
    "colors = [f'C{i}' for i in cmask ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1421a094-8224-4ab5-9ef4-dc85295d8e79",
   "metadata": {},
   "outputs": [],
   "source": [
    "nrows = 2\n",
    "ncols = 5\n",
    "\n",
    "fig, ax = plt.subplots(nrows,ncols,figsize=(16,9))\n",
    "\n",
    "ax[0,0].scatter(*Z.T, color=colors)\n",
    "ax[0,0].set_title('Norm(0,1)',fontsize=14)\n",
    "\n",
    "'''\n",
    "Let each subsequent step of the flow viz the jazz\n",
    "'''\n",
    "\n",
    "Z_gpu = Z.float().to(device)\n",
    "\n",
    "for k,module in enumerate(modules):\n",
    "    \n",
    "    with torch.no_grad():    \n",
    "        Z_gpu, _ = module(Z_gpu)\n",
    "    \n",
    "    \n",
    "    i = (k+1) // ncols\n",
    "    j = (k+1) % ncols\n",
    "    \n",
    "    ax[i,j].set_title(f'Flow step {k+1}',fontsize=14)\n",
    "    \n",
    "    ax[i,j].scatter(*Z_gpu.cpu().numpy().T, color=colors)\n",
    "      \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2af543b4-26fa-4e6e-8d1a-ef3ff75eb203",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "453b6b8b-5b2c-47f2-a73a-088aa5a34aab",
   "metadata": {},
   "source": [
    "**Resources:**\n",
    "- This tutorial for the code in this repo came from the [pytorch-flows](https://github.com/ikostrikov/pytorch-flows) repo.\n",
    "- The [nflows](https://github.com/bayesiains/nflows.git) is also a very nice package that includes the Real-NVP model and also the RQ-NSF that we also talked about in the lecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "728dc6cf-fb65-429a-9119-648d6095b2df",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
